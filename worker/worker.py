import os, time, json, traceback
from threading import Thread
import psutil
from redis import Redis
from PIL import Image
import numpy as np
from sentence_transformers import SentenceTransformer
from transformers import BlipProcessor, BlipForConditionalGeneration
import faiss
import torch

# è®€å– Worker åç¨±
WORKER_NAME = os.getenv("WORKER_NAME", "unknown")

# è³‡æ–™ç›®éŒ„èˆ‡ Redis key è¨­å®š
UPLOAD_DIR = "/data/uploads"
META_PATH = "/data/metadata.json"
INDEX_PATH = "/data/index_file.index"
QUEUE = "image_queue"
PROCESSING_SET = "processing_set"
DONE_SET = "done_set"

# Metrics Hash åç¨±
METRICS_HASH = "node_metrics"

# é€£ç·š Redis
redis = Redis(host="redis", port=6379, decode_responses=True)

device = "cuda" if torch.cuda.is_available() else "cpu"

# æ¨¡å‹è¼‰å…¥
caption_processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
caption_model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base").to(device)
embedder = SentenceTransformer("all-MiniLM-L6-v2")

# åˆå§‹åŒ– metadata å’Œ index
if os.path.exists(META_PATH):
    with open(META_PATH, "r", encoding="utf-8") as f:
        metadata = json.load(f)
else:
    metadata = []
if os.path.exists(INDEX_PATH):
    index = faiss.read_index(INDEX_PATH)
else:
    dim = embedder.get_sentence_embedding_dimension()
    index = faiss.IndexFlatL2(dim)

# è™•ç†è¶…æ™‚ç§’æ•¸ï¼ˆä¿ç•™æ“´å……ç”¨ï¼‰
PROCESSING_TIMEOUT = 60

# Metrics ä¸Šå ±ï¼šå®šæœŸå°‡ CPU% èˆ‡ Memory% å¯«å…¥ Redis hash
def publish_metrics():
    while True:
        # psutil.cpu_percent(interval=1) æœƒé˜»å¡ 1 ç§’æ¡æ¨£
        cpu = psutil.cpu_percent(interval=1)
        mem = psutil.virtual_memory().percent
        timestamp = time.time()
        data = {"cpu": cpu, "mem": mem, "ts": timestamp}
        try:
            redis.hset(METRICS_HASH, WORKER_NAME, json.dumps(data))
        except Exception:
            # å¦‚æœ‰éŒ¯èª¤å°±æ‰“å°ï¼Œä½†ä¸å½±éŸ¿ä¸»å¾ªç’°
            print(f"âš ï¸ Failed to publish metrics: {traceback.format_exc()}")
        time.sleep(4)  # å‰©é¤˜æ™‚é–“ç¡çœ 

# å•Ÿå‹• metrics thread
Thread(target=publish_metrics, daemon=True).start()

print(f"Worker '{WORKER_NAME}' started on {device} device")

# ä¸»å¾ªç’°ï¼šè™•ç†ä»»å‹™
while True:
    try:
        image_path = redis.rpop(QUEUE)
        if not image_path:
            time.sleep(1)
            continue

        print(f"ğŸ”„ Processing image: {image_path} by {WORKER_NAME}")

        # æ¨™è¨˜è™•ç†ä¸­ä¸¦è¨˜éŒ„æ˜¯å“ªä¸€å°
        redis.sadd(PROCESSING_SET, image_path)
        redis.hset("processing_workers", image_path, WORKER_NAME)
        start_time = time.time()
        full_path = os.path.join("/data", image_path)

        try:
            if not os.path.exists(full_path) or not os.path.isfile(full_path):
                raise FileNotFoundError(f"File not found: {full_path}")

            image = Image.open(full_path).convert("RGB")

            # ç”Ÿæˆæè¿°
            inputs = caption_processor(image, return_tensors="pt").to(device)
            out = caption_model.generate(**inputs, max_length=50)
            caption = caption_processor.decode(out[0], skip_special_tokens=True)

            # ç”Ÿæˆå‘é‡
            vec = embedder.encode(caption).astype(np.float32)

            # åŠ é–å¯« metadata å’Œ FAISS
            with redis.lock("write_lock", timeout=10):
                # è®€ metadata
                if os.path.exists(META_PATH):
                    with open(META_PATH, "r", encoding="utf-8") as f:
                        metadata = json.load(f)
                else:
                    metadata = []

                # è®€ index
                if os.path.exists(INDEX_PATH):
                    index = faiss.read_index(INDEX_PATH)
                else:
                    dim = embedder.get_sentence_embedding_dimension()
                    index = faiss.IndexFlatL2(dim)

                # æ›´æ–° metadata
                metadata.append({
                    "filename": image_path,
                    "caption": caption
                })
                with open(META_PATH, "w", encoding="utf-8") as f:
                    json.dump(metadata, f, ensure_ascii=False, indent=2)

                # æ›´æ–° FAISS
                index.add(np.array([vec]))
                faiss.write_index(index, INDEX_PATH)

            # è™•ç†å®Œæˆï¼šç§»é™¤ processing è¨˜éŒ„ã€åŠ å…¥ done ä¸¦æ¸… processing_workers
            redis.srem(PROCESSING_SET, image_path)
            redis.hdel("processing_workers", image_path)
            redis.sadd(DONE_SET, image_path)

            processing_time = time.time() - start_time
            print(f"âœ… {WORKER_NAME} processed {image_path} in {processing_time:.2f}s: {caption}")

        except Exception as e:
            error_msg = f"âŒ Error processing {image_path} by {WORKER_NAME}: {str(e)}"
            print(error_msg)
            print(traceback.format_exc())

            # æ¸…ç† processing set
            redis.srem(PROCESSING_SET, image_path)
            redis.set(f"error:{image_path}", error_msg)

            # é‡è©¦ä¸€æ¬¡
            if not redis.get(f"retry:{image_path}"):
                print(f"ğŸ”„ Requeueing {image_path} for retry")
                redis.set(f"retry:{image_path}", "1")
                redis.lpush(QUEUE, image_path)
            else:
                print(f"âŒ Failed to process {image_path} after retry")

    except Exception as e:
        print(f"âš ï¸ Worker main loop error: {str(e)}")
        print(traceback.format_exc())
        time.sleep(5)
