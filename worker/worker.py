import os, time, json, traceback, atexit
from threading import Thread
import psutil
from redis import Redis
from PIL import Image
import numpy as np
from sentence_transformers import SentenceTransformer
from transformers import BlipProcessor, BlipForConditionalGeneration
import faiss
import torch
from pillow_heif import register_heif_opener
import piexif
from geopy.geocoders import Nominatim
import base64
from io import BytesIO
import cohere
import random

from dotenv import load_dotenv
load_dotenv()

# è®€å– Worker åç¨±
WORKER_NAME = os.getenv("WORKER_NAME", "unknown")

# è³‡æ–™ç›®éŒ„èˆ‡ Redis key è¨­å®š
UPLOAD_DIR = "/data/uploads"

# å°‡å–®ä¸€queueæ›æˆprefix
QUEUE_PREFIX = "image_queue"
PROCESSING_SET_PREFIX = "processing_set"
DONE_SET_PREFIX = "done_set"

# Metrics Hash åç¨±
METRICS_HASH = "node_metrics"

HEARTBEAT_KEY     = f"heartbeat:{WORKER_NAME}"
HEARTBEAT_EXPIRE  = 5    # å¿ƒè·³ key éæœŸæ™‚é–“ (ç§’)
HEARTBEAT_INTERVAL= 1     # å¿ƒè·³æ›´æ–°é–“éš” (ç§’)

register_heif_opener()
geolocator = Nominatim(user_agent="image-rag")

def dms_to_decimal(dms, ref):
    degrees = dms[0][0] / dms[0][1]
    minutes = dms[1][0] / dms[1][1]
    seconds = dms[2][0] / dms[2][1]
    decimal = degrees + minutes / 60 + seconds / 3600
    if ref in [b'S', b'W']:
        decimal *= -1
    return round(decimal, 6)

# é€£ç·š Redis
redis = Redis(host="redis", port=6379, decode_responses=True)

# å°‡è‡ªå·±è¨»å†Šåˆ° active_workers set è£¡ï¼Œç›£æ§ç¨‹å¼å¯ç”¨ä¾†çŸ¥é“å“ªäº›ç¯€é»ä¸Šç·š
redis.sadd("active_workers", WORKER_NAME)
# é‡æ–°é–‹æ©Ÿå°±é¦¬ä¸Šé€å‡ºç¬¬ä¸€é¡†å¿ƒè·³
redis.set(HEARTBEAT_KEY, time.time(), ex=HEARTBEAT_EXPIRE)

def on_exit():
    redis.srem("active_workers", WORKER_NAME)
atexit.register(on_exit)

def publish_heartbeat():
    while True:
        # æ¯ HEARTBEAT_INTERVAL ç§’æ›´æ–°ä¸€æ¬¡ï¼Œä¸¦è¨­å®šè‡ªå‹•éæœŸ
        redis.set(HEARTBEAT_KEY, time.time(), ex=HEARTBEAT_EXPIRE)
        time.sleep(HEARTBEAT_INTERVAL)

# Metrics ä¸Šå ±ï¼šå®šæœŸå°‡ CPU% èˆ‡ Memory% å¯«å…¥ Redis hash
def publish_metrics():
    while True:
        # psutil.cpu_percent(interval=1) æœƒé˜»å¡ 1 ç§’æ¡æ¨£
        cpu = psutil.cpu_percent(interval=1)
        mem = psutil.virtual_memory().percent
        timestamp = time.time()
        data = {"cpu": cpu, "mem": mem, "ts": timestamp}
        try:
            redis.hset(METRICS_HASH, WORKER_NAME, json.dumps(data))
        except Exception:
            print(f"âš ï¸ Failed to publish metrics: {traceback.format_exc()}")
        time.sleep(2)  # å‰©é¤˜æ™‚é–“ç¡çœ 

# å•Ÿå‹•èƒŒæ™¯thread
Thread(target=publish_heartbeat, daemon=True).start()
Thread(target=publish_metrics, daemon=True).start()

device = "cuda" if torch.cuda.is_available() else "cpu"
# æ¨¡å‹è¼‰å…¥
COHERE_API_KEY = os.getenv("COHERE_API_KEY")
co = cohere.ClientV2(api_key=COHERE_API_KEY)

caption_processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
caption_model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base").to(device)
embedder = SentenceTransformer("all-MiniLM-L6-v2")

# è¼‰å…¥ metadata å’Œ index
# ä¸éœ€è¦é å…ˆè¼‰å…¥å…¨åŸŸmetadataå’Œindex
print(f"Worker '{WORKER_NAME}' started on {device} device")

# ä¸åœå¾ªç’°å¾ redis çš„ image_queue æ‹¿ä»»å‹™å‡ºä¾†åš
while True:
    try:
        # 1) å…ˆå¾ Redis æ‹¿å‡ºæ‰€æœ‰ active_users
        user_ids = list(redis.smembers("active_users"))
        # 2) è¨ˆç®—æ¯å€‹ user çš„éšŠåˆ—é•·åº¦
        lengths = []
        total = 0
        for u in user_ids:
            l = redis.llen(f"{QUEUE_PREFIX}:{u}")
            if l > 0:
                lengths.append((u, l))
                total += l
        # å¦‚æœæ²’æœ‰ä»»ä½•ä»»å‹™ï¼Œsleep ç„¶å¾Œç¹¼çºŒ
        if total == 0:
            time.sleep(0.1)
            continue

        # 3) æŒ‰ length åŠ æ¬Šéš¨æ©Ÿé¸ä¸€å€‹ user
        #    ï¼ˆéšŠåˆ—è¶Šé•·è¢«é¸ä¸­çš„æ©Ÿç‡è¶Šå¤§ï¼‰
        r = random.uniform(0, total)
        upto = 0
        for (u, l) in lengths:
            upto += l
            if upto >= r:
                selected_user = u
                break

        queue_key = f"{QUEUE_PREFIX}:{selected_user}"
        image_path = redis.rpop(queue_key)
        if not image_path:
            # é€™å€‹æ¥µå°‘ç™¼ç”Ÿï¼Œé‡è©¦ä¸€ä¸‹
            continue

        # --- æ‹¿åˆ°ä¸€æ¢çœŸæ­£è¦è™•ç†çš„ä»»å‹™ï¼Œä¸‹é¢æ²¿ç”¨ä½ ç¾æœ‰çš„é‚è¼¯ï¼Œåªæ˜¯æŠŠ `user` æ›æˆ selected_user ---
        user = selected_user
        start_time = time.time()
        orig_image_path = image_path  # <== æ–°å¢ï¼šè¨˜ä½åŸä¾†çš„è·¯å¾‘
        redis.set(f"processing_ts:{user}:{image_path}", start_time)
        
        print(f"ğŸ”„ Processing image: {image_path} for user {user} by {WORKER_NAME}")

        # æ¨™è¨˜è™•ç†ä¸­ä¸¦è¨˜éŒ„æ˜¯å“ªä¸€å°
        redis.sadd(f"{PROCESSING_SET_PREFIX}:{user}", image_path)
        redis.hset("processing_workers", f"{user}:{image_path}", WORKER_NAME)
        
        # ä½¿ç”¨è€…ç‰¹å®šçš„è·¯å¾‘
        user_meta = f"/data/metadata_{user}.json"
        user_index = f"/data/index_file_{user}.index"
        user_pdf_index = f"/data/pdf_index_{user}.index"
        user_pdf_meta = f"/data/pdf_metadata_{user}.json"
        
        full_path = os.path.join("/data", image_path)
        try:
            if not os.path.exists(full_path) or not os.path.isfile(full_path):
                raise FileNotFoundError(f"File not found: {full_path}")

            # é–‹å•Ÿåœ–ç‰‡
            image = Image.open(full_path).convert("RGB")

            # å‹•æ…‹åˆ¤æ–·ï¼šæ˜¯ä¸æ˜¯ä¸Šå‚³åˆ° uploads/{user}/pdfs ä¸‹çš„æª”æ¡ˆ
            pdf_folder = f"uploads/{user}/pdfs"
            # æŠŠå…©é‚Šéƒ½æ¨™æº–åŒ–ä¸€ä¸‹å†æ¯”
            norm_image = os.path.normpath(image_path)
            norm_folder = os.path.normpath(pdf_folder)
            print(f"[DEBUG] user={user} pdf_folder={norm_folder} image_path={norm_image}")
            is_pdf_page = norm_image.startswith(norm_folder)

            if is_pdf_page:
                print(f"ğŸ“„ Processing PDF image with Cohere: {image_path}")

                # è½‰æˆ base64 URL
                buf = BytesIO()
                image.save(buf, format="JPEG")
                b64 = base64.b64encode(buf.getvalue()).decode("utf-8")
                b64_url = f"data:image/jpeg;base64,{b64}"

                image_input = {
                    "content": [
                        {"type": "image_url", "image_url": {"url": b64_url}}
                    ]
                }

                try:
                    res = co.embed(
                        model="embed-v4.0",
                        inputs=[image_input],
                        input_type="search_document",
                        embedding_types=["float"]
                    )
                    vector = np.array(res.embeddings.float_[0], dtype=np.float32)

                    with redis.lock(f"pdf_write_lock:{user}", timeout=10):
                        if os.path.exists(user_pdf_index):
                            pdf_index = faiss.read_index(user_pdf_index)
                        else:
                            pdf_index = faiss.IndexFlatL2(len(vector))

                        if os.path.exists(user_pdf_meta):
                            with open(user_pdf_meta, "r", encoding="utf-8") as f:
                                pdf_meta = json.load(f)
                        else:
                            pdf_meta = []

                        pdf_index.add(np.array([vector]))
                        faiss.write_index(pdf_index, user_pdf_index)

                        pdf_meta.append({"filename": image_path})
                        json.dump(pdf_meta, open(user_pdf_meta, "w", encoding="utf-8"), ensure_ascii=False, indent=2)

                    # æ¨™è¨˜å®Œæˆ
                    redis.delete(f"processing_ts:{user}:{orig_image_path}")
                    redis.srem(f"{PROCESSING_SET_PREFIX}:{user}", orig_image_path)
                    redis.hdel("processing_workers", f"{user}:{orig_image_path}")
                    redis.sadd(f"{DONE_SET_PREFIX}:{user}", orig_image_path)

                    print(f"âœ… {WORKER_NAME} done {image_path} for user {user} with Cohere embedding")

                    continue  # â—ï¸é€™ä¸€é»å¾ˆé‡è¦ï¼Œè·³éé è¨­ BLIP è™•ç†

                except Exception as e:
                    print(f"âŒ Cohere embedding failed for {image_path}: {e}")
                    raise e

            # ç”¨ BLIP ç”Ÿ caption
            inputs = caption_processor(image, return_tensors="pt").to(device)
            out = caption_model.generate(**inputs, max_length=50)
            caption = caption_processor.decode(out[0], skip_special_tokens=True)

            # åŠ é–å¯« metadata å’Œ FAISS
            with redis.lock(f"write_lock:{user}", timeout=10):
                """
                worker æ‹¿åˆ°é–ä¹‹å¾Œ
                é¦¬ä¸Šè®€ã€Œç¾åœ¨æœ€æ–°ç£ç¢Ÿä¸Šçš„ metadata.jsonã€indexã€
                åŸºæ–¼æœ€æ–°ç‰ˆæœ¬å»åŠ è‡ªå·±çš„æ–°è³‡æ–™
                ä»¥å…è¦†è“‹æ‰å…¶ä»– worker çš„è³‡æ–™
                """
                # è®€ä½¿ç”¨è€…çš„ metadata
                if os.path.exists(user_meta):
                    metadata = json.load(open(user_meta, "r", encoding="utf-8"))
                else:
                    metadata = []

                # è®€ä½¿ç”¨è€…çš„ index
                if os.path.exists(user_index):
                    index = faiss.read_index(user_index)
                else:
                    dim = embedder.get_sentence_embedding_dimension()
                    index = faiss.IndexFlatL2(dim)

                # æ›´æ–° metadata
                # é è¨­æ¬„ä½
                country = None
                city = None
                date_str = None

                # è‹¥ç‚º HEICï¼Œå…ˆæå– metadataï¼Œå†è½‰æˆ JPG ä¸¦è¦†è“‹
                if image_path.lower().endswith(".heic"):
                    try:
                        img = Image.open(full_path)

                        # æå– EXIF
                        exif_bytes = img.info.get("exif")
                        if exif_bytes:
                            exif_dict = piexif.load(exif_bytes)

                            # æ™‚é–“
                            date_bytes = exif_dict.get("Exif", {}).get(piexif.ExifIFD.DateTimeOriginal)
                            if date_bytes:
                                date_str = date_bytes.decode(errors="ignore").split(" ")[0].replace(":", "-")

                            # GPS
                            gps = exif_dict.get("GPS", {})
                            lat = gps.get(piexif.GPSIFD.GPSLatitude)
                            lat_ref = gps.get(piexif.GPSIFD.GPSLatitudeRef)
                            lon = gps.get(piexif.GPSIFD.GPSLongitude)
                            lon_ref = gps.get(piexif.GPSIFD.GPSLongitudeRef)

                            if lat and lat_ref and lon and lon_ref:
                                lat_decimal = dms_to_decimal(lat, lat_ref)
                                lon_decimal = dms_to_decimal(lon, lon_ref)
                                location = geolocator.reverse((lat_decimal, lon_decimal), language="en", timeout=10)
                                if location and "address" in location.raw:
                                    addr = location.raw["address"]
                                    country = addr.get("country")
                                    city = addr.get("city", addr.get("town", addr.get("village")))
                        else:
                            print(f"âŒ No EXIF found: {image_path}")

                        # âœ… è½‰æˆ JPG ä¸¦è¦†è“‹ï¼šuploads/foo.heic â†’ uploads/foo.jpg
                        base_name = os.path.splitext(image_path)[0]  # uploads/foo
                        new_rel_path = base_name + ".jpg"
                        new_abs_path = os.path.join("/data", new_rel_path)

                        img.convert("RGB").save(new_abs_path, "JPEG")

                        # åˆªé™¤åŸå§‹ .heic
                        os.remove(full_path)

                        # ä¿å­˜èˆŠçš„.heicè·¯å¾‘ç”¨æ–¼ç§»é™¤
                        orig_heic_path = orig_image_path  # æš«å­˜åŸå§‹.heicè·¯å¾‘
                        
                        # æ›¿æ› image_path èˆ‡ full_path ç‚ºæ–°çš„ .jpg
                        image_path = new_rel_path
                        full_path = new_abs_path
                        
                        # --- HEIC âœ JPG æˆåŠŸå¾ŒåŒæ­¥æ›´æ–°æ‰€æœ‰Redis keys ---
                        # 0. æº–å‚™æ–°èˆŠkey
                        old_key = f"{user}:{orig_heic_path}"
                        new_key = f"{user}:{new_rel_path}"
                        
                        # 1. ç§»é™¤èˆŠ processing æ¨™è¨˜
                        redis.delete(f"processing_ts:{user}:{orig_heic_path}")
                        redis.srem(f"{PROCESSING_SET_PREFIX}:{user}", orig_heic_path)
                        redis.hdel("processing_workers", old_key)
                        
                        # 2. åŠ å…¥æ–° processing æ¨™è¨˜
                        redis.set(f"processing_ts:{user}:{new_rel_path}", time.time())
                        redis.sadd(f"{PROCESSING_SET_PREFIX}:{user}", new_rel_path)
                        redis.hset("processing_workers", new_key, WORKER_NAME)
                        
                        # 3. æ›´æ–°è®Šæ•¸ï¼Œè®“å¾Œé¢æ¸…ç† / done_set éƒ½ç”¨ .jpg
                        orig_image_path = new_rel_path  # å¾ŒçºŒ finally/æ¸…ç†ç”¨
                        
                        # 4. done_setè™•ç†
                        redis.sadd(f"{DONE_SET_PREFIX}:{user}", new_rel_path)
                        redis.srem(f"{DONE_SET_PREFIX}:{user}", orig_heic_path)
                        
                        print(f"ğŸ–¼ï¸ HEIC converted and replaced: {image_path}")

                    except Exception as e:
                        print(f"âš ï¸ HEIC metadata or convert failed: {e}")

                # é€™è£¡æ·»åŠ ç§»å‹•éä¾†çš„å‘é‡ç”Ÿæˆä»£ç¢¼
                full_text = f"{caption}. Location: {city}, {country}. Date: {date_str or ''}."
                vec = embedder.encode(full_text).astype(np.float32)

                entry = {
                    "filename": image_path,
                    "caption": caption
                }
                if country:
                    entry["country"] = country
                if city:
                    entry["city"] = city
                if date_str:
                    entry["date"] = date_str

                metadata.append(entry)
                json.dump(metadata, open(user_meta, "w", encoding="utf-8"), ensure_ascii=False, indent=2)

                # æ›´æ–° FAISS
                index.add(np.array([vec]))
                faiss.write_index(index, user_index)

            # è™•ç†å®Œæˆï¼šç§»é™¤ processing è¨˜éŒ„ã€åŠ å…¥ done ä¸¦æ¸… processing_workers
            redis.delete(f"processing_ts:{user}:{orig_image_path}")
            redis.srem(f"{PROCESSING_SET_PREFIX}:{user}", orig_image_path)
            redis.hdel("processing_workers", f"{user}:{orig_image_path}")
            redis.sadd(f"{DONE_SET_PREFIX}:{user}", orig_image_path)

            elapsed = time.time() - start_time
            print(f"âœ… {WORKER_NAME} done {image_path} for user {user} in {elapsed:.2f}s: {caption}")

        except Exception as e:
            # è™•ç†å¤±æ•—ï¼šæ¸…è™•ç†æ™‚é–“ï¼Œè¨˜éŒ„ errorï¼Œä¸¦åšä¸€æ¬¡ retry
            error_msg = f"âŒ Error processing {image_path} for user {user} by {WORKER_NAME}: {str(e)}"
            print(error_msg)
            print(traceback.format_exc())

            # æ¸…ç† processing set
            redis.delete(f"processing_ts:{user}:{orig_image_path}")
            redis.srem(f"{PROCESSING_SET_PREFIX}:{user}", orig_image_path)
            redis.set(f"error:{user}:{orig_image_path}", error_msg)

            # é‡è©¦ä¸€æ¬¡
            if not redis.get(f"retry:{user}:{orig_image_path}"):
                print(f"ğŸ”„ Requeueing {image_path} for user {user} for retry")
                redis.set(f"retry:{user}:{orig_image_path}", "1")
                redis.lpush(f"{QUEUE_PREFIX}:{user}", image_path)
            else:
                print(f"âŒ Failed to process {image_path} for user {user} after retry")

    except Exception as e:
        print(f"âš ï¸ Worker main loop error: {str(e)}")
        print(traceback.format_exc())
        time.sleep(5)